## Audio-Description-of-Scenes-for-the-Visually-Impaired
# Introduction
We delve into image captioning, sentiment analysis, and text-to-speech platforms.  We attempted to create a tool that would allow those who are visually impaired or challenged to get an idea of their surroundings by an audio description of any scene. We use a vision language model to caption an image, analyze the sentiment, and convert it to audio in the language of the end-users’ choosing. Our primary target contribution is to application, we extend the use of a language model used for short captions to generations of paragraphs based on an image. Supporting functionality, such as language translation is added to the model to support a pipeline that takes in an image as input, and produces the resultant captioned audio as output. We’ve thus far been able to implement all parts of this framework, from the image captioning, to the text to speech generation. While the latter part of the pipeline works smoothly, while evaluating the image captioning model that was built for shorter caption generations using similarity indexes such as BLEU score, the captions may not always form fully coherent sentences but do pick up a vast majority of the key details in the image.

# Implementation

We initially use InceptionV3 (which is pretrained on Imagenet) to classify each image and extract features from the last convolutional layer. The input images are resized and converted to the InceptionV3's expected format. They are then preprocessed to normalize the pixel values in the range -1 to 1. The output from this model is then cached using a caching strategy.
The text captions for the images are transformed into integer sequences using the TextVectorization layer. We iterate over all captions, split the captions into words, and compute a vocabulary of the top 5,000 words (to save memory). The captions are then tokenized by mapping each word to its index in the vocabulary. All output sequences are padded to length 50. The word-to-index and index-to-word mappings are used to display results.
The final model architecture uses the features extracted from the lower convolutional layer of InceptionV3 to produce a vector that is resized and then passed into a CNN encoder which consists of a single fully connected layer. The RNN which is a GRU in this case attends over the image to predict the next word.
